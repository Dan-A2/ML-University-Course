{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./sharif.png\" alt=\"SUT logo\" width=300 height=300 align=left class=\"saturate\">\n",
    "\n",
    "<br>\n",
    "<font>\n",
    "<div dir=ltr align=center>\n",
    "<font color=0F5298 size=7>\n",
    "    Machine Learning <br>\n",
    "<font color=2565AE size=5>\n",
    "    Computer Engineering Department <br>\n",
    "    Spring 2024<br>\n",
    "<font color=3C99D size=5>\n",
    "    Practical Assignment 3<br>\n",
    "<font color=696880 size=4>\n",
    "    Ashkan Majidi - Shayan Salehi - Amirhossein Alamdar\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_number = '99100455'\n",
    "first_name = 'Danial'\n",
    "last_name = 'Ataee'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this assignment, you will implement SVM (Support Vector Machines) for classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prepfocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import your needed libraries in following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix\n",
    "import cvxopt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.spatial import distance\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from ```satimage``` dataset and split data to features and labels. The ```label``` column is our target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.5</td>\n",
       "      <td>6.272730</td>\n",
       "      <td>4.047619</td>\n",
       "      <td>3.975208</td>\n",
       "      <td>3.032258</td>\n",
       "      <td>4.545455</td>\n",
       "      <td>8.421050</td>\n",
       "      <td>1.8125</td>\n",
       "      <td>-2.2500</td>\n",
       "      <td>0.077672</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-0.138462</td>\n",
       "      <td>-0.188119</td>\n",
       "      <td>-0.431579</td>\n",
       "      <td>-0.546875</td>\n",
       "      <td>-0.15625</td>\n",
       "      <td>-0.126214</td>\n",
       "      <td>-0.431579</td>\n",
       "      <td>-0.484375</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.5</td>\n",
       "      <td>4.090909</td>\n",
       "      <td>-0.714290</td>\n",
       "      <td>1.776858</td>\n",
       "      <td>2.903226</td>\n",
       "      <td>3.090909</td>\n",
       "      <td>5.052630</td>\n",
       "      <td>-1.7500</td>\n",
       "      <td>-2.2500</td>\n",
       "      <td>0.077672</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-0.015385</td>\n",
       "      <td>-0.049505</td>\n",
       "      <td>-0.431579</td>\n",
       "      <td>-0.609375</td>\n",
       "      <td>-0.15625</td>\n",
       "      <td>-0.126214</td>\n",
       "      <td>-0.494737</td>\n",
       "      <td>-0.609375</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>9.909090</td>\n",
       "      <td>8.333330</td>\n",
       "      <td>5.479339</td>\n",
       "      <td>3.354839</td>\n",
       "      <td>7.272730</td>\n",
       "      <td>10.526316</td>\n",
       "      <td>2.7500</td>\n",
       "      <td>3.4375</td>\n",
       "      <td>6.368924</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.233333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.207921</td>\n",
       "      <td>-0.010526</td>\n",
       "      <td>-0.312500</td>\n",
       "      <td>-0.15625</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>-0.326316</td>\n",
       "      <td>-0.437500</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.5</td>\n",
       "      <td>4.818182</td>\n",
       "      <td>1.190480</td>\n",
       "      <td>2.702482</td>\n",
       "      <td>2.774194</td>\n",
       "      <td>2.363640</td>\n",
       "      <td>5.894740</td>\n",
       "      <td>-0.2500</td>\n",
       "      <td>-3.8750</td>\n",
       "      <td>-2.368924</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.383333</td>\n",
       "      <td>-0.138462</td>\n",
       "      <td>-0.049505</td>\n",
       "      <td>-0.347368</td>\n",
       "      <td>-0.484375</td>\n",
       "      <td>0.09375</td>\n",
       "      <td>0.087379</td>\n",
       "      <td>-0.031579</td>\n",
       "      <td>-0.218750</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.5</td>\n",
       "      <td>4.818182</td>\n",
       "      <td>2.142860</td>\n",
       "      <td>2.702482</td>\n",
       "      <td>3.032258</td>\n",
       "      <td>3.090909</td>\n",
       "      <td>6.736840</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>-0.6250</td>\n",
       "      <td>-1.320388</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.183333</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.267327</td>\n",
       "      <td>-0.031579</td>\n",
       "      <td>-0.281250</td>\n",
       "      <td>-0.03125</td>\n",
       "      <td>-0.126214</td>\n",
       "      <td>-0.431579</td>\n",
       "      <td>-0.546875</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1         2         3         4         5          6       7  \\\n",
       "0   7.5  6.272730  4.047619  3.975208  3.032258  4.545455   8.421050  1.8125   \n",
       "1   7.5  4.090909 -0.714290  1.776858  2.903226  3.090909   5.052630 -1.7500   \n",
       "2  12.0  9.909090  8.333330  5.479339  3.354839  7.272730  10.526316  2.7500   \n",
       "3   7.5  4.818182  1.190480  2.702482  2.774194  2.363640   5.894740 -0.2500   \n",
       "4   7.5  4.818182  2.142860  2.702482  3.032258  3.090909   6.736840 -1.0000   \n",
       "\n",
       "        8         9  ...        27        28        29        30        31  \\\n",
       "0 -2.2500  0.077672  ... -0.600000 -0.138462 -0.188119 -0.431579 -0.546875   \n",
       "1 -2.2500  0.077672  ... -0.600000 -0.015385 -0.049505 -0.431579 -0.609375   \n",
       "2  3.4375  6.368924  ... -0.233333  0.200000  0.207921 -0.010526 -0.312500   \n",
       "3 -3.8750 -2.368924  ... -0.383333 -0.138462 -0.049505 -0.347368 -0.484375   \n",
       "4 -0.6250 -1.320388  ... -0.183333  0.230769  0.267327 -0.031579 -0.281250   \n",
       "\n",
       "        32        33        34        35  label  \n",
       "0 -0.15625 -0.126214 -0.431579 -0.484375    6.0  \n",
       "1 -0.15625 -0.126214 -0.494737 -0.609375    6.0  \n",
       "2 -0.15625  0.009709 -0.326316 -0.437500    6.0  \n",
       "3  0.09375  0.087379 -0.031579 -0.218750    6.0  \n",
       "4 -0.03125 -0.126214 -0.431579 -0.546875    6.0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./Data/satimage.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 2., 1., 5., 3., 4.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split data to train, validation and test parts. 60% of data should be used for train, 15% for validation and 25% for test. After that scale the data to Standard Normal Distribution using ```StandardScaler``` class from ```scikit-learn``` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data['label'] == 4) | (data['label'] == 6)]\n",
    "\n",
    "X = data.drop(['label'], axis=1).values\n",
    "y = data['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.fit_transform(X_val)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train Binary SVM model for classification between labels 4 and 6. Choose corresponding datas and convert their lables to 1 and -1 respectively for 4 and 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(y_train == 4, 1, -1)\n",
    "y_val = np.where(y_val == 4, 1, -1)\n",
    "y_test = np.where(y_test == 4, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is soft margin svm convex optimization formulation. \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text { Minimize } & \\frac{1}{2}\\|\\mathbf{w}\\|^2+C \\sum_{i=1}^N \\xi_i \\\\\n",
    "\\text { Subject to } & y_i\\left(\\mathbf{w}^T \\mathbf{x}_i+b\\right) \\geq 1-\\xi_i, \\quad i=1,2, \\ldots, N \\\\\n",
    "& \\xi_i \\geq 0, \\quad i=1,2, \\ldots, N\n",
    "\\end{aligned}\n",
    "$$\n",
    "write dual of the soft margin svm optimization problem below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Maximize}\\ \\ \\sum_{n=1}^{N}\\alpha_n-\\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{N}\n",
    "\\alpha_n\\alpha_my^{(n)}y^{(m)}x^{{(n)}^T}x^{(m)}\n",
    "$$\n",
    "$$\n",
    "\\text{s.t.}\n",
    "\\begin{cases}\n",
    "    \\sum\\limits_{n=1}^{N}\\alpha_ny^{(n)}=0 \\\\\n",
    "    0\\leq\\alpha_n\\leq C & n=1,\\dots,N\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should write this optimization problem in standard Quadratic Program (QP) form and use a QP solver to find optimal answer. Here is General form of a QP:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text { Minimize } & \\frac{1}{2} x^TPx + q^Tx + r \\\\\n",
    "\\text { Subject to } & Gx \\leq h \\\\\n",
    "& Ax = b\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Complete functions below and use ```cvxopt``` library which is a common library for solving QPs. Note that you can't use ```scikit-learn``` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_margin_svm(X, y, C):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        X: training data\n",
    "        y: training labels\n",
    "        C: errors weight\n",
    "    output:\n",
    "        support_vectros: data points which are SVs of our model.\n",
    "        support_vector_labels: labels of SVs\n",
    "        support_vector_alphas: alpha coefficient of corresponding SVs \n",
    "    \"\"\"\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # Gram matrix\n",
    "    K = np.dot(X, X.T)\n",
    "\n",
    "    # P, q, G, h, A, b matrices for cvxopt\n",
    "    P = cvxopt.matrix(np.outer(y, y) * K)\n",
    "    q = cvxopt.matrix(-np.ones(n_samples))\n",
    "    G = cvxopt.matrix(np.vstack((np.eye(n_samples) * -1, np.eye(n_samples))))\n",
    "    h = cvxopt.matrix(np.hstack((np.zeros(n_samples), np.ones(n_samples) * C)))\n",
    "    A = cvxopt.matrix(y, (1, n_samples), 'd')\n",
    "    b = cvxopt.matrix(0.0)\n",
    "\n",
    "    # Solve QP problem\n",
    "    solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "    # Lagrange multipliers\n",
    "    alphas = np.array(solution['x']).flatten()\n",
    "\n",
    "    # Support vectors have non-zero lagrange multipliers\n",
    "    sv = (alphas > 1e-5)\n",
    "    ind = np.arange(len(alphas))[sv]\n",
    "    support_vectors = X[sv]\n",
    "    support_vector_labels = y[sv]\n",
    "    support_vector_alphas = alphas[sv]\n",
    "\n",
    "    return support_vectors, support_vector_labels, support_vector_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -6.1347e+02 -3.3324e+03  2e+04  3e+00  4e-13\n",
      " 1: -4.2280e+02 -2.2442e+03  3e+03  4e-01  2e-13\n",
      " 2: -3.9327e+02 -8.2039e+02  5e+02  4e-02  2e-13\n",
      " 3: -4.3173e+02 -5.6725e+02  2e+02  1e-02  2e-13\n",
      " 4: -4.5093e+02 -5.2320e+02  8e+01  5e-03  2e-13\n",
      " 5: -4.5957e+02 -5.0423e+02  5e+01  2e-03  2e-13\n",
      " 6: -4.6654e+02 -4.9024e+02  2e+01  1e-03  2e-13\n",
      " 7: -4.7068e+02 -4.8224e+02  1e+01  3e-04  2e-13\n",
      " 8: -4.7349e+02 -4.7802e+02  5e+00  1e-04  2e-13\n",
      " 9: -4.7454e+02 -4.7643e+02  2e+00  4e-05  2e-13\n",
      "10: -4.7502e+02 -4.7573e+02  7e-01  1e-05  2e-13\n",
      "11: -4.7528e+02 -4.7540e+02  1e-01  1e-06  2e-13\n",
      "12: -4.7533e+02 -4.7534e+02  8e-03  8e-08  2e-13\n",
      "13: -4.7533e+02 -4.7534e+02  1e-04  1e-09  2e-13\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "C = 1.0\n",
    "support_vectors, support_vector_labels, support_vector_alphas = soft_margin_svm(X_train, y_train, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(x_test, support_vectors, support_vector_labels, support_vector_alphas):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        x_test: test data\n",
    "        support_vectros: data points which are SVs of our model.\n",
    "        support_vector_labels: labels of SVs\n",
    "        support_vector_alphas: alpha coefficient of corresponding SVs \n",
    "    output:\n",
    "        y_pred: predictoin labels\n",
    "    \"\"\"\n",
    "    \n",
    "    K = np.dot(support_vectors, x_test.T)\n",
    "    decision_func = np.dot((support_vector_labels * support_vector_alphas).reshape(1, -1), K)\n",
    "    y_pred = np.sign(decision_func).flatten()\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(np.array(X_test), support_vectors, support_vector_labels, support_vector_alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, balanced accuracy and plot confusion matrix of your trained model using ```Seaborn.heatmap()``` function. Use case of balanced accuracy is when dealing with imbalanced data, i.e. when one of the target classes appears a lot more than the other and it is defined by average of recall of classes. Confusion matrix is a $k\\times k$ (k is number of classes) matrix which cell ij is showing that number of data points labeld i which predicted j. For more detail of evaluation metrics you can see [here](https://neptune.ai/blog/balanced-accuracy).\n",
    "\n",
    "You may use ```scikit-learn``` library to compute these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7883895131086143\n",
      "Balanced Accuracy:  0.8292613636363637\n",
      "Confusion Matrix:\n",
      " [[272 102]\n",
      " [ 11 149]]\n"
     ]
    }
   ],
   "source": [
    "def evaluate(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        y_test: true labels\n",
    "        y_pred: predicted labels\n",
    "    output:\n",
    "        accuracy: accuracy of the model\n",
    "        balanced_accuracy: balanced accuracy of the model\n",
    "        confusion_matrix: confusion matrix of the model\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Balanced Accuracy: \", balanced_accuracy)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "    \n",
    "    return accuracy, balanced_accuracy, confusion_mat\n",
    "\n",
    "_, _, confusion_mat = evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_matrix = pd.DataFrame(data=confusion_mat, columns=['Predict Negative:-1', 'Predict Positive:1'],\n",
    "                                 index=['Actual Negative:-1', 'Actual Positive:1'])\n",
    "\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='mako')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tunning (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "like what you did before, implement Soft Margin SVM but now use rbf kernel. To determine rbf $\\gamma$ parameter use validation datas and find best(best by balanced accuracy) $\\gamma$ between 0.001 , 0.01, 1, 10, 100. Not that you can't use ```scikit-learn``` library here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf(x1, x2, gamma=1):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        x1: first vector\n",
    "        x2: second vector\n",
    "        gamma: rbf kernel parameter with defalt value 1.\n",
    "    output:\n",
    "        ouput: computed rbf of two input vectors\n",
    "    \"\"\"\n",
    "    diff = np.subtract(x1, x2)\n",
    "    squared_norm = np.dot(diff, diff)\n",
    "    return np.exp(-gamma * squared_norm)\n",
    "\n",
    "def soft_margin_svm(X, y, C, gamma=1):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        X: training data\n",
    "        y: training labels\n",
    "        C: errors weight\n",
    "        gamma: rbf kernel parameter with defalt value 1.\n",
    "    output:\n",
    "        support_vectros: data points which are SVs of our model.\n",
    "        support_vector_labels: labels of SVs\n",
    "        support_vector_alphas: alpha coefficient of corresponding SVs \n",
    "    \"\"\"\n",
    "\n",
    "    n = X.shape[0]\n",
    "    K = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            K[i, j] = rbf(X[i], X[j], gamma)\n",
    "    P = cvxopt.matrix(np.outer(y, y) * K)\n",
    "    q = cvxopt.matrix(-np.ones(n))\n",
    "    G = cvxopt.matrix(np.vstack((-np.eye(n), np.eye(n))))\n",
    "    h = cvxopt.matrix(np.hstack((np.zeros(n), C * np.ones(n))))\n",
    "    A = cvxopt.matrix(y, (1, n), 'd')\n",
    "    b = cvxopt.matrix(0.0)\n",
    "\n",
    "    solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "    alphas = np.ravel(solution['x'])\n",
    "\n",
    "    sv = (alphas > 1e-5)\n",
    "    support_vectors = X[sv]\n",
    "    support_vector_labels = y[sv]\n",
    "    support_vector_alphas = alphas[sv]\n",
    "\n",
    "    return support_vectors, support_vector_labels, support_vector_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solving for gamma=0.1:\n",
      "\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.3534e+02 -2.5450e+03  1e+04  2e+00  1e-15\n",
      " 1: -1.9153e+02 -1.4528e+03  2e+03  1e-01  1e-15\n",
      " 2: -2.0922e+02 -4.4909e+02  3e+02  2e-02  1e-15\n",
      " 3: -2.3314e+02 -3.0204e+02  7e+01  4e-03  1e-15\n",
      " 4: -2.4426e+02 -2.5935e+02  2e+01  1e-04  1e-15\n",
      " 5: -2.4679e+02 -2.5164e+02  5e+00  2e-05  1e-15\n",
      " 6: -2.4746e+02 -2.4986e+02  2e+00  9e-06  1e-15\n",
      " 7: -2.4781e+02 -2.4895e+02  1e+00  1e-14  1e-15\n",
      " 8: -2.4808e+02 -2.4832e+02  2e-01  2e-14  1e-15\n",
      " 9: -2.4814e+02 -2.4820e+02  6e-02  3e-15  1e-15\n",
      "10: -2.4816e+02 -2.4816e+02  3e-03  1e-14  1e-15\n",
      "11: -2.4816e+02 -2.4816e+02  4e-05  1e-14  1e-15\n",
      "Optimal solution found.\n",
      "Accuracy:  0.803125\n",
      "Balanced Accuracy:  0.8421912746909195\n",
      "Confusion Matrix:\n",
      " [[170  57]\n",
      " [  6  87]]\n",
      "\n",
      "solving for gamma=0.1:\n",
      "\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.3534e+02 -2.5450e+03  1e+04  2e+00  1e-15\n",
      " 1: -1.9153e+02 -1.4528e+03  2e+03  1e-01  1e-15\n",
      " 2: -2.0922e+02 -4.4909e+02  3e+02  2e-02  1e-15\n",
      " 3: -2.3314e+02 -3.0204e+02  7e+01  4e-03  1e-15\n",
      " 4: -2.4426e+02 -2.5935e+02  2e+01  1e-04  1e-15\n",
      " 5: -2.4679e+02 -2.5164e+02  5e+00  2e-05  1e-15\n",
      " 6: -2.4746e+02 -2.4986e+02  2e+00  9e-06  1e-15\n",
      " 7: -2.4781e+02 -2.4895e+02  1e+00  1e-14  1e-15\n",
      " 8: -2.4808e+02 -2.4832e+02  2e-01  2e-14  1e-15\n",
      " 9: -2.4814e+02 -2.4820e+02  6e-02  3e-15  1e-15\n",
      "10: -2.4816e+02 -2.4816e+02  3e-03  1e-14  1e-15\n",
      "11: -2.4816e+02 -2.4816e+02  4e-05  1e-14  1e-15\n",
      "Optimal solution found.\n",
      "Accuracy:  0.803125\n",
      "Balanced Accuracy:  0.8421912746909195\n",
      "Confusion Matrix:\n",
      " [[170  57]\n",
      " [  6  87]]\n",
      "\n",
      "solving for gamma=1:\n",
      "\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.2056e+02 -2.1469e+03  2e+03  7e-14  1e-15\n",
      " 1: -4.2872e+02 -5.3943e+02  1e+02  1e-14  5e-16\n",
      " 2: -4.3849e+02 -4.4480e+02  6e+00  1e-14  2e-16\n",
      " 3: -4.3940e+02 -4.3966e+02  3e-01  2e-15  2e-16\n",
      " 4: -4.3946e+02 -4.3946e+02  8e-03  5e-15  1e-16\n",
      " 5: -4.3946e+02 -4.3946e+02  3e-04  1e-14  1e-16\n",
      "Optimal solution found.\n",
      "Accuracy:  0.803125\n",
      "Balanced Accuracy:  0.8358438728624887\n",
      "Confusion Matrix:\n",
      " [[172  55]\n",
      " [  8  85]]\n",
      "\n",
      "solving for gamma=10:\n",
      "\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.6988e+02 -2.1491e+03  2e+03  1e-13  4e-16\n",
      " 1: -4.7668e+02 -5.1378e+02  4e+01  9e-14  2e-16\n",
      " 2: -4.8273e+02 -4.8314e+02  4e-01  4e-15  8e-17\n",
      " 3: -4.8280e+02 -4.8281e+02  4e-03  2e-14  1e-16\n",
      " 4: -4.8280e+02 -4.8280e+02  4e-05  4e-14  8e-17\n",
      "Optimal solution found.\n",
      "Accuracy:  0.803125\n",
      "Balanced Accuracy:  0.8358438728624887\n",
      "Confusion Matrix:\n",
      " [[172  55]\n",
      " [  8  85]]\n",
      "\n",
      "solving for gamma=100:\n",
      "\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.6988e+02 -2.1491e+03  2e+03  1e-13  2e-16\n",
      " 1: -4.7668e+02 -5.1378e+02  4e+01  2e-13  4e-16\n",
      " 2: -4.8273e+02 -4.8314e+02  4e-01  5e-14  7e-17\n",
      " 3: -4.8280e+02 -4.8281e+02  4e-03  6e-14  6e-17\n",
      " 4: -4.8280e+02 -4.8280e+02  4e-05  3e-14  5e-17\n",
      "Optimal solution found.\n",
      "Accuracy:  0.803125\n",
      "Balanced Accuracy:  0.8358438728624887\n",
      "Confusion Matrix:\n",
      " [[172  55]\n",
      " [  8  85]]\n",
      "\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "gammas = [00.1, 0.1, 1, 10, 100]\n",
    "best_gamma = None\n",
    "best_balanced_accuracy = -1\n",
    "best_support_vectors = None\n",
    "best_support_vector_labels = None\n",
    "best_support_vector_alphas = None\n",
    "\n",
    "for gamma in gammas:\n",
    "    print(\"solving for gamma={}:\\n\".format(gamma))\n",
    "    support_vectors, support_vector_labels, support_vector_alphas = soft_margin_svm(X_train, y_train, C, gamma)\n",
    "    y_pred = predict_labels(np.array(X_val), support_vectors, support_vector_labels, support_vector_alphas)\n",
    "    _, ba, _ = evaluate(y_val, y_pred)\n",
    "    print()\n",
    "    if ba > best_balanced_accuracy:\n",
    "        best_gamma = gamma\n",
    "        best_balanced_accuracy = ba\n",
    "        best_support_vectors = support_vectors\n",
    "        best_support_vector_labels = support_vector_labels\n",
    "        best_support_vector_alphas = support_vector_alphas\n",
    "\n",
    "print(best_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7902621722846442\n",
      "Balanced Accuracy:  0.8323863636363636\n",
      "Confusion Matrix:\n",
      " [[272 102]\n",
      " [ 10 150]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_labels(np.array(X_test), best_support_vectors, best_support_vector_labels, best_support_vector_alphas)\n",
    "_, _, confusion_mat = evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass SVM (30 points + 50 points optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want implement Multiclass SVM. Use ```SVC``` function with rbf kernel from ```scikit-learn``` package on all train datas (all six classes) and choose best hyperparameters for $C$ and $\\gamma$ between 0.01, 0.1, 1, 10, 100. After that evaluate it with your function.\n",
    "\n",
    "Note that in this example we had enough data to split them in train and validation but in case that we don't have enough data, ```scikit-learn``` has a built-in fast library named ```GridSearchCV()``` which can help us in hyperparameter tunning with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./Data/satimage.csv')\n",
    "\n",
    "X = data.drop(['label'], axis=1).values\n",
    "y = data['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.625)\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.fit_transform(X_val)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 10\n",
      "Best Gamma: 0.1\n",
      "Accuracy: 0.9179614667495338\n",
      "Balanced Accuracy: 0.8925105909690897\n",
      "Confusion Matrix:\n",
      " [[391   0   5   0   0   0]\n",
      " [  0 167   0   0   1   0]\n",
      " [  2   1 313  10   0   4]\n",
      " [  0   2  38  88   2  27]\n",
      " [  2   1   0   0 166   7]\n",
      " [  0   0   8  15   7 352]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9179614667495338,\n",
       " 0.8925105909690897,\n",
       " array([[391,   0,   5,   0,   0,   0],\n",
       "        [  0, 167,   0,   0,   1,   0],\n",
       "        [  2,   1, 313,  10,   0,   4],\n",
       "        [  0,   2,  38,  88,   2,  27],\n",
       "        [  2,   1,   0,   0, 166,   7],\n",
       "        [  0,   0,   8,  15,   7, 352]], dtype=int64))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cs = [0.01, 0.1, 1, 10, 100]\n",
    "gammas = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "    confusion_mat = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Balanced Accuracy:\", balanced_accuracy)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "    return accuracy, balanced_accuracy, confusion_mat\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = {'C': Cs, 'gamma': gammas}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Train the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_C = grid_search.best_params_['C']\n",
    "best_gamma = grid_search.best_params_['gamma']\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best C:\", best_C)\n",
    "print(\"Best Gamma:\", best_gamma)\n",
    "\n",
    "# Predict labels on test data using the model with best hyperparameters\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Multiclass SVM from scratch without using ready functions (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    linear = lambda x, x_ , c=0: x @ x_ .T\n",
    "    polynomial = lambda x, x_ , Q=5: (1 + x @ x_.T)**Q\n",
    "    rbf = lambda x, x_ , gamma=10: np.exp(-gamma * distance.cdist(x, x_,'sqeuclidean'))\n",
    "    kernel_funs = {'linear': linear, 'polynomial': polynomial, 'rbf': rbf}\n",
    "    \n",
    "    def __init__(self, kernel='rbf', C=1, k=2):\n",
    "        # set the hyperparameters\n",
    "        self.kernel_str = kernel\n",
    "        self.kernel = SVM.kernel_funs[kernel]\n",
    "        self.C = C                  # regularization parameter\n",
    "        self.k = k                  # kernel parameter\n",
    "        \n",
    "        # training data and support vectors\n",
    "        self.X, y = None, None\n",
    "        self.alphas = None\n",
    "        \n",
    "        # for multi-class classification\n",
    "        self.multiclass = False\n",
    "        self.clfs = []                                  \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if len(np.unique(y)) > 2:\n",
    "            self.multiclass = True\n",
    "            return self.multi_fit(X, y)\n",
    "        \n",
    "        # relabel if needed\n",
    "        if set(np.unique(y)) == {0, 1}: y[y == 0] = -1\n",
    "        # ensure y has dimensions Nx1\n",
    "        self.y = y.reshape(-1, 1).astype(np.double) # Has to be a column vector\n",
    "        self.X = X\n",
    "        N = X.shape[0]\n",
    "        \n",
    "        # compute the kernel over all possible pairs of (x, x') in the data\n",
    "        self.K = self.kernel(X, X, self.k)\n",
    "        \n",
    "        # For 1/2 x^T P x + q^T x\n",
    "        P = cvxopt.matrix(self.y @ self.y.T * self.K)\n",
    "        q = cvxopt.matrix(-np.ones((N, 1)))\n",
    "        \n",
    "        # For Ax = b\n",
    "        A = cvxopt.matrix(self.y.T)\n",
    "        b = cvxopt.matrix(np.zeros(1))\n",
    "\n",
    "        # For Gx <= h\n",
    "        G = cvxopt.matrix(np.vstack((-np.identity(N),\n",
    "                                    np.identity(N))))\n",
    "        h = cvxopt.matrix(np.vstack((np.zeros((N,1)),\n",
    "                                    np.ones((N,1)) * self.C)))\n",
    "\n",
    "        # Solve    \n",
    "        cvxopt.solvers.options['show_progress'] = False\n",
    "        sol = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        self.alphas = np.array(sol[\"x\"])\n",
    "            \n",
    "        # Maps into support vectors\n",
    "        self.is_sv = ((self.alphas > 1e-3) & (self.alphas <= self.C)).squeeze()\n",
    "        self.margin_sv = np.argmax((1e-3 < self.alphas) & (self.alphas < self.C - 1e-3))\n",
    "\n",
    "    def multi_fit(self, X, y):\n",
    "        self.k = len(np.unique(y))      # number of classes\n",
    "        # for each pair of classes\n",
    "        for i in range(self.k):\n",
    "            # get the data for the pair\n",
    "            Xs, Ys = X, copy.copy(y)\n",
    "            # change the labels to -1 and 1\n",
    "            Ys[Ys!=i], Ys[Ys==i] = -1, +1\n",
    "            # fit the classifier\n",
    "            clf = SVM(kernel=self.kernel_str, C=self.C, k=self.k)\n",
    "            clf.fit(Xs, Ys)\n",
    "            # save the classifier\n",
    "            self.clfs.append(clf)\n",
    "\n",
    "    def predict(self, X_t):\n",
    "        if self.multiclass: return self.multi_predict(X_t)\n",
    "        xₛ, yₛ = self.X[self.margin_sv, np.newaxis], self.y[self.margin_sv]\n",
    "        alphas, y, X= self.alphas[self.is_sv], self.y[self.is_sv], self.X[self.is_sv]\n",
    "\n",
    "        b = yₛ - np.sum(alphas * y * self.kernel(X, xₛ, self.k), axis=0)\n",
    "        score = np.sum(alphas * y * self.kernel(X, X_t, self.k), axis=0) + b\n",
    "        return np.sign(score).astype(int), score\n",
    "\n",
    "    def multi_predict(self, X):\n",
    "        # get the predictions from all classifiers\n",
    "        preds = np.zeros((X.shape[0], self.k))\n",
    "        for i, clf in enumerate(self.clfs):\n",
    "            _, preds[:, i] = clf.predict(X)\n",
    "        \n",
    "        # get the argmax and the corresponding score\n",
    "        return np.argmax(preds, axis=1)\n",
    "\n",
    "    def evaluate(self, X,y):  \n",
    "        outputs, _ = self.predict(X)\n",
    "        accuracy = np.sum(outputs == y) / len(y)\n",
    "        return round(accuracy, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8614045991298943\n",
      "Balanced Accuracy: 0.8172144215635978\n",
      "Confusion Matrix:\n",
      " [[391   0   3   0   2   0]\n",
      " [  0 161   0   0   7   0]\n",
      " [  1   0 309  18   1   1]\n",
      " [  1   2  44  54   5  51]\n",
      " [  6  10   0   1 145  14]\n",
      " [  0   0  11  31  14 326]]\n"
     ]
    }
   ],
   "source": [
    "my_svm = SVM(k=6)\n",
    "my_svm.fit(X_train, y_train)\n",
    "y_pred = my_svm.multi_predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy)\n",
    "print(\"Confusion Matrix:\\n\", confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different SVM Kernels (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A kernel in SVM is like a tool that helps solve tricky problems. It lets us work in a space with many dimensions, making complex calculations easier. With kernels, we can deal with lots of dimensions, even an endless amount. Kernels are crucial for sorting data into groups and help spot patterns in the data we're looking at. They're especially good at tackling twisty problems with a straightforward approach.\n",
    "\n",
    "Sometimes, finding a straight line or flat surface to divide data isn't possible, especially as we explore more dimensions. That's where different types of SVM kernels come in handy. They transform twisty, complicated data into a simpler form that's easier to separate. In this explanation, we talk about 4 popular types of these kernels. Also in following cells you can use any package.\n",
    "\n",
    "<img src=\"./kernel.jpg\" alt=\"Types of Kernel Functions\"  align=center class=\"saturate\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the simplest kind of kernel, typically working in one dimension. It works best when dealing with a lot of features. Linear kernels are quicker than other types.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$F(x, x_j) = sum(x, x_j)$$\n",
    "\n",
    "Now Implement svc classifier using a linear kernel. Get the prediction and evaluate it by function which you implemented before. Also plot confusion matrix by `Seaborn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8551895587321318\n",
      "Balanced Accuracy: 0.8094841942932366\n",
      "Confusion Matrix:\n",
      " [[390   0   4   0   2   0]\n",
      " [  1 158   0   1   8   0]\n",
      " [  2   0 308  19   0   1]\n",
      " [  1   2  43  53   5  53]\n",
      " [  8   9   0   1 143  15]\n",
      " [  0   0  10  32  16 324]]\n"
     ]
    }
   ],
   "source": [
    "C = 10\n",
    "\n",
    "svc_linear = SVC(kernel='linear', C=C)\n",
    "svc_linear.fit(X_train, y_train)\n",
    "y_pred = svc_linear.predict(X_test)\n",
    "_, _, confusion_mat = evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian RBF kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel is a favorite choice in SVM, especially for data that doesn't line up straight. It's great for sorting data when you don't know much about it beforehand.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$F(x, x_j) = \\exp{(-\\gamma ||x - xj||^2)}$$\n",
    "\n",
    "Now, configure the SVC classifier with a sigmoid kernel. Get the prediction and evaluate it by function which you implemented before. Also plot confusion matrix as previous part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9179614667495338\n",
      "Balanced Accuracy: 0.8925105909690897\n",
      "Confusion Matrix:\n",
      " [[391   0   5   0   0   0]\n",
      " [  0 167   0   0   1   0]\n",
      " [  2   1 313  10   0   4]\n",
      " [  0   2  38  88   2  27]\n",
      " [  2   1   0   0 166   7]\n",
      " [  0   0   8  15   7 352]]\n"
     ]
    }
   ],
   "source": [
    "svc_rbf = SVC(kernel='rbf', C=C, gamma=0.1)\n",
    "svc_rbf.fit(X_train, y_train)\n",
    "y_pred = svc_rbf.predict(X_test)\n",
    "_, _, confusion_mat = evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a broader version of the linear kernel. It's not as popular because it's not as quick or precise.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$F(x, x_j) = (x.x_j+1)^d$$\n",
    "\n",
    "Now, set up the SVC classifier using a *polynomial* kernel. Get the prediction and evaluate it by function which you implemented before. Also plot confusion matrix as previous parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9049098819142325\n",
      "Balanced Accuracy: 0.8827016843582355\n",
      "Confusion Matrix:\n",
      " [[391   0   4   0   1   0]\n",
      " [  0 165   0   0   3   0]\n",
      " [  5   0 304  12   0   9]\n",
      " [  1   3  31  93   2  27]\n",
      " [  2   2   0   1 162   9]\n",
      " [  0   1   9  21  10 341]]\n"
     ]
    }
   ],
   "source": [
    "svc_poly = SVC(kernel='poly', C=C, degree=6, coef0=1)\n",
    "svc_poly.fit(X_train, y_train)\n",
    "y_pred = svc_poly.predict(X_test)\n",
    "_, _, confusion_mat = evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is mainly chosen for use with neural networks. The kernel function acts like the activation function in a two-layer perceptron neural network model, helping to activate the neurons.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$F(x, x_j) = \\tanh(α x a y + c)$$\n",
    "\n",
    "Now, configure the SVC classifier with a sigmoid kernel. Get the prediction and evaluate it by function which you implemented before. Also plot confusion matrix as previous parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.48788067122436296\n",
      "Balanced Accuracy: 0.3882636994429188\n",
      "Confusion Matrix:\n",
      " [[294  42  47  13   0   0]\n",
      " [134  27   4   0   2   1]\n",
      " [ 48   0 268   7   3   4]\n",
      " [  2   0 102  18   3  32]\n",
      " [ 43  46   6  35  11  35]\n",
      " [  2   0  42 154  17 167]]\n"
     ]
    }
   ],
   "source": [
    "svc_sigmoid = SVC(kernel='sigmoid', C=C, gamma=1, coef0=0)\n",
    "svc_sigmoid.fit(X_train, y_train)\n",
    "y_pred = svc_sigmoid.predict(X_test)\n",
    "_, _, confusion_mat = evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these four kernel functions with each other. What are the main advantages and disadvantages of each one? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Analysis\n",
    "\n",
    "After tuning the models, we can compare their performance by their accuracy and balanced accuracy. Linear, polynomial, and RBF kernels had the highest accuracy/balanced accuracy (88% in average) with very close results, with RBF having the best performance. Sigmoid operated significantly poorer than the other three with extremely low accuracy (~50%) and balanced accuracy (~40%).\n",
    "\n",
    "## Comparing Kernels\n",
    "\n",
    "### Linear Kernel:\n",
    "\n",
    "**Characteristics:**\n",
    "- The linear kernel is the simplest form of kernel function used in SVM.\n",
    "- It computes the dot product between two feature vectors.\n",
    "- Decision boundary is a straight line in the input space.\n",
    "\n",
    "**Performance:**\n",
    "- Linear kernels work well when the data is linearly separable.\n",
    "- They are computationally efficient and fast to train.\n",
    "- They perform best when the relationship between features and labels is close to linear.\n",
    "\n",
    "**Suitability:**\n",
    "- Suitable for datasets with a large number of features and a linear relationship between features and labels.\n",
    "- Effective when the decision boundary is expected to be close to linear.\n",
    "\n",
    "### RBF (Radial Basis Function) Kernel:\n",
    "\n",
    "**Characteristics:**\n",
    "- The RBF kernel computes the similarity between two feature vectors based on the Gaussian (or Radial Basis) function.\n",
    "- It is capable of capturing complex, nonlinear relationships in the data.\n",
    "- Decision boundary is non-linear and can adapt to the shape of the data.\n",
    "\n",
    "**Performance:**\n",
    "- RBF kernels are highly flexible and can model complex decision boundaries.\n",
    "- They perform well on datasets with non-linear relationships and are suitable for high-dimensional data.\n",
    "- However, they are more prone to overfitting and require careful tuning of hyperparameters.\n",
    "\n",
    "**Suitability:**\n",
    "- Suitable for datasets with complex and non-linear relationships.\n",
    "- Effective when the decision boundary is expected to be non-linear and adaptive to the data distribution.\n",
    "\n",
    "### Polynomial Kernel:\n",
    "\n",
    "**Characteristics:**\n",
    "- The polynomial kernel computes the similarity between two feature vectors using polynomial functions.\n",
    "- It can learn decision boundaries of various shapes by setting higher polynomial degrees.\n",
    "- Decision boundary can have different degrees depending on the polynomial degree.\n",
    "\n",
    "**Performance:**\n",
    "- Polynomial kernels can capture non-linear relationships in the data.\n",
    "- They perform well when the decision boundary has a polynomial shape.\n",
    "- However, they are sensitive to the choice of hyperparameters and can be computationally expensive, especially for higher polynomial degrees.\n",
    "\n",
    "**Suitability:**\n",
    "- Suitable for datasets with non-linear relationships where the decision boundary has a polynomial shape.\n",
    "- Effective when the relationship between features and labels can be represented by polynomial functions.\n",
    "\n",
    "### Sigmoid Kernel:\n",
    "\n",
    "**Characteristics:**\n",
    "- The sigmoid kernel computes the similarity between two feature vectors using sigmoid functions.\n",
    "- It can capture non-linear relationships and is useful for binary classification tasks.\n",
    "- Decision boundary is shaped by the sigmoid function.\n",
    "\n",
    "**Performance:**\n",
    "- Sigmoid kernels can capture non-linear relationships, but they often perform poorly compared to other kernels.\n",
    "- They are less common in practice due to their limited applicability and tendency to underperform.\n",
    "- Sensitive to the choice of hyperparameters and may require careful tuning.\n",
    "\n",
    "**Suitability:**\n",
    "- Suitable for binary classification tasks with non-linearly separable data.\n",
    "- Effective when dealing with non-linear relationships and when the decision boundary can be shaped by sigmoid functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
